{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c11e97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from squeezenet_qat.pth...\n",
      "\n",
      "======================================================================\n",
      "STATE DICT STRUCTURE\n",
      "======================================================================\n",
      "conv1.bias                                         (96,)               \n",
      "conv1.weight                                       (96, 3, 7, 7)       \n",
      "conv10.bias                                        (10,)               \n",
      "conv10.weight                                      (10, 512, 1, 1)     \n",
      "fire2.conv1.bias                                   (16,)               \n",
      "fire2.conv1.weight                                 (16, 96, 1, 1)      \n",
      "fire2.conv2.bias                                   (64,)               \n",
      "fire2.conv2.weight                                 (64, 16, 1, 1)      \n",
      "fire2.conv3.bias                                   (64,)               \n",
      "fire2.conv3.weight                                 (64, 16, 3, 3)      \n",
      "fire3.conv1.bias                                   (16,)               \n",
      "fire3.conv1.weight                                 (16, 128, 1, 1)     \n",
      "fire3.conv2.bias                                   (64,)               \n",
      "fire3.conv2.weight                                 (64, 16, 1, 1)      \n",
      "fire3.conv3.bias                                   (64,)               \n",
      "fire3.conv3.weight                                 (64, 16, 3, 3)      \n",
      "fire4.conv1.bias                                   (32,)               \n",
      "fire4.conv1.weight                                 (32, 128, 1, 1)     \n",
      "fire4.conv2.bias                                   (128,)              \n",
      "fire4.conv2.weight                                 (128, 32, 1, 1)     \n",
      "fire4.conv3.bias                                   (128,)              \n",
      "fire4.conv3.weight                                 (128, 32, 3, 3)     \n",
      "fire5.conv1.bias                                   (32,)               \n",
      "fire5.conv1.weight                                 (32, 256, 1, 1)     \n",
      "fire5.conv2.bias                                   (128,)              \n",
      "fire5.conv2.weight                                 (128, 32, 1, 1)     \n",
      "fire5.conv3.bias                                   (128,)              \n",
      "fire5.conv3.weight                                 (128, 32, 3, 3)     \n",
      "fire6.conv1.bias                                   (48,)               \n",
      "fire6.conv1.weight                                 (48, 256, 1, 1)     \n",
      "fire6.conv2.bias                                   (192,)              \n",
      "fire6.conv2.weight                                 (192, 48, 1, 1)     \n",
      "fire6.conv3.bias                                   (192,)              \n",
      "fire6.conv3.weight                                 (192, 48, 3, 3)     \n",
      "fire7.conv1.bias                                   (48,)               \n",
      "fire7.conv1.weight                                 (48, 384, 1, 1)     \n",
      "fire7.conv2.bias                                   (192,)              \n",
      "fire7.conv2.weight                                 (192, 48, 1, 1)     \n",
      "fire7.conv3.bias                                   (192,)              \n",
      "fire7.conv3.weight                                 (192, 48, 3, 3)     \n",
      "fire8.conv1.bias                                   (64,)               \n",
      "fire8.conv1.weight                                 (64, 384, 1, 1)     \n",
      "fire8.conv2.bias                                   (256,)              \n",
      "fire8.conv2.weight                                 (256, 64, 1, 1)     \n",
      "fire8.conv3.bias                                   (256,)              \n",
      "fire8.conv3.weight                                 (256, 64, 3, 3)     \n",
      "fire9.conv1.bias                                   (64,)               \n",
      "fire9.conv1.weight                                 (64, 512, 1, 1)     \n",
      "fire9.conv2.bias                                   (256,)              \n",
      "fire9.conv2.weight                                 (256, 64, 1, 1)     \n",
      "fire9.conv3.bias                                   (256,)              \n",
      "fire9.conv3.weight                                 (256, 64, 3, 3)     \n",
      "======================================================================\n",
      "\n",
      "Extracting weights...\n",
      "\n",
      "Extracting Conv1 weights...\n",
      "  Found: conv1.weight\n",
      "  Shape: (96, 3, 7, 7)\n",
      "\n",
      "Extracting Fire module weights...\n",
      "No fire modules found with pattern 'fire' in keys\n",
      "Attempting fallback: checking features.X for Fire-like structure...\n",
      "\n",
      "Extracting Fire2 weights...\n",
      "Warning: Could not find squeeze weights for patterns ['features.3', 'features.fire2']\n",
      "\n",
      "Extracting Fire3 weights...\n",
      "Warning: Could not find squeeze weights for patterns ['features.4', 'features.fire3']\n",
      "\n",
      "Extracting Fire4 weights...\n",
      "Warning: Could not find squeeze weights for patterns ['features.5', 'features.fire4']\n",
      "\n",
      "Extracting Fire5 weights...\n",
      "Warning: Could not find squeeze weights for patterns ['features.7', 'features.fire5']\n",
      "\n",
      "Extracting Fire6 weights...\n",
      "Warning: Could not find squeeze weights for patterns ['features.8', 'features.fire6']\n",
      "\n",
      "Extracting Fire7 weights...\n",
      "Warning: Could not find squeeze weights for patterns ['features.9', 'features.fire7']\n",
      "\n",
      "Extracting Fire8 weights...\n",
      "Warning: Could not find squeeze weights for patterns ['features.10', 'features.fire8']\n",
      "\n",
      "Extracting Fire9 weights...\n",
      "Warning: Could not find squeeze weights for patterns ['features.12', 'features.fire9']\n",
      "\n",
      "Extracting Conv10 weights...\n",
      "  Found: conv10.weight\n",
      "  Shape: (10, 512, 1, 1)\n",
      "\n",
      "Writing to weights.h...\n",
      "✓ Successfully extracted weights to weights.h\n",
      "\n",
      "Next steps:\n",
      "1. Compile the project: make weight_load\n",
      "2. Run the test: ./tb_weight_loading\n",
      "3. Verify weight values match your expected quantization\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Extract quantized weights from PyTorch checkpoint and generate C++ header file.\n",
    "Usage: python extract_weights.py squeeze_qat.pth\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def explore_state_dict(state_dict):\n",
    "    \"\"\"Print all keys in the state dict to understand structure\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STATE DICT STRUCTURE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    keys = sorted(state_dict.keys())\n",
    "    for key in keys:\n",
    "        shape = tuple(state_dict[key].shape)\n",
    "        print(f\"{key:50s} {str(shape):20s}\")\n",
    "    \n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    return keys\n",
    "\n",
    "def quantize_to_fixed_point(weight, int_bits=4, frac_bits=4):\n",
    "    \"\"\"\n",
    "    Convert float weights to fixed-point representation.\n",
    "    Total bits = int_bits + frac_bits\n",
    "    \"\"\"\n",
    "    total_bits = int_bits + frac_bits\n",
    "    scale = 2 ** frac_bits\n",
    "    \n",
    "    # Quantize\n",
    "    quantized = np.round(weight * scale).astype(np.int32)\n",
    "    \n",
    "    # Saturate to valid range\n",
    "    max_val = (2 ** (total_bits - 1)) - 1\n",
    "    min_val = -(2 ** (total_bits - 1))\n",
    "    quantized = np.clip(quantized, min_val, max_val)\n",
    "    \n",
    "    return quantized\n",
    "\n",
    "def find_layer_key(state_dict, patterns):\n",
    "    \"\"\"Find the first matching key for given patterns\"\"\"\n",
    "    keys = state_dict.keys()\n",
    "    for pattern in patterns:\n",
    "        for key in keys:\n",
    "            if pattern in key and key.endswith('.weight'):\n",
    "                return key\n",
    "    return None\n",
    "\n",
    "def format_array_cpp(name, data, line_width=12):\n",
    "    \"\"\"Format numpy array as C++ array initialization\"\"\"\n",
    "    flat = data.flatten()\n",
    "    lines = []\n",
    "    lines.append(f\"const fixed_point_t {name}[{len(flat)}] = {{\")\n",
    "    \n",
    "    # Format in rows\n",
    "    for i in range(0, len(flat), line_width):\n",
    "        row = flat[i:i+line_width]\n",
    "        row_str = \", \".join(f\"{int(x)}\" for x in row)\n",
    "        lines.append(f\"    {row_str},\")\n",
    "    \n",
    "    # Remove trailing comma from last line\n",
    "    if lines[-1].endswith(\",\"):\n",
    "        lines[-1] = lines[-1][:-1]\n",
    "    \n",
    "    lines.append(\"};\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def extract_conv_weights(state_dict, layer_patterns, out_name):\n",
    "    \"\"\"Extract convolutional layer weights with flexible key matching\"\"\"\n",
    "    # Try multiple possible key patterns\n",
    "    if isinstance(layer_patterns, str):\n",
    "        layer_patterns = [layer_patterns]\n",
    "    \n",
    "    key = find_layer_key(state_dict, [p + '.weight' for p in layer_patterns])\n",
    "    \n",
    "    if key is None:\n",
    "        print(f\"Warning: Could not find weight key matching {layer_patterns}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Found: {key}\")\n",
    "    weight = state_dict[key].cpu().numpy()\n",
    "    print(f\"  Shape: {weight.shape}\")\n",
    "    quantized = quantize_to_fixed_point(weight)\n",
    "    return format_array_cpp(out_name, quantized)\n",
    "\n",
    "def extract_fire_weights(state_dict, fire_patterns):\n",
    "    \"\"\"Extract all weights from a Fire module with flexible pattern matching\"\"\"\n",
    "    \n",
    "    # Try to find squeeze weights\n",
    "    squeeze_key = find_layer_key(state_dict, [p + '.squeeze' for p in fire_patterns])\n",
    "    if not squeeze_key:\n",
    "        print(f\"Warning: Could not find squeeze weights for patterns {fire_patterns}\")\n",
    "        return None\n",
    "    \n",
    "    # Try to find expand1x1 weights  \n",
    "    expand1_key = find_layer_key(state_dict, [p + '.expand1x1' for p in fire_patterns])\n",
    "    if not expand1_key:\n",
    "        print(f\"Warning: Could not find expand1x1 weights for patterns {fire_patterns}\")\n",
    "        return None\n",
    "    \n",
    "    # Try to find expand3x3 weights\n",
    "    expand3_key = find_layer_key(state_dict, [p + '.expand3x3' for p in fire_patterns])\n",
    "    if not expand3_key:\n",
    "        print(f\"Warning: Could not find expand3x3 weights for patterns {fire_patterns}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Squeeze: {squeeze_key} {state_dict[squeeze_key].shape}\")\n",
    "    print(f\"  Expand1: {expand1_key} {state_dict[expand1_key].shape}\")\n",
    "    print(f\"  Expand3: {expand3_key} {state_dict[expand3_key].shape}\")\n",
    "    \n",
    "    squeeze = state_dict[squeeze_key].cpu().numpy()\n",
    "    squeeze_quant = quantize_to_fixed_point(squeeze)\n",
    "    \n",
    "    expand1 = state_dict[expand1_key].cpu().numpy()\n",
    "    expand1_quant = quantize_to_fixed_point(expand1)\n",
    "    \n",
    "    expand3 = state_dict[expand3_key].cpu().numpy()\n",
    "    expand3_quant = quantize_to_fixed_point(expand3)\n",
    "    \n",
    "    # Extract fire number from the pattern\n",
    "    fire_num = None\n",
    "    for pattern in fire_patterns:\n",
    "        if 'fire' in pattern.lower():\n",
    "            # Extract number from pattern like \"fire2\" or \"features.fire2\"\n",
    "            import re\n",
    "            match = re.search(r'fire(\\d+)', pattern.lower())\n",
    "            if match:\n",
    "                fire_num = match.group(1)\n",
    "                break\n",
    "    \n",
    "    if fire_num is None:\n",
    "        fire_num = \"unknown\"\n",
    "    \n",
    "    fire_name = f\"fire{fire_num}\"\n",
    "    \n",
    "    results = []\n",
    "    results.append(format_array_cpp(f\"{fire_name}_squeeze_weights_flat\", squeeze_quant))\n",
    "    results.append(format_array_cpp(f\"{fire_name}_expand1x1_weights_flat\", expand1_quant))\n",
    "    results.append(format_array_cpp(f\"{fire_name}_expand3x3_weights_flat\", expand3_quant))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    checkpoint_path = \"squeezenet_qat.pth\"\n",
    "    \n",
    "    if not Path(checkpoint_path).exists():\n",
    "        print(f\"Error: File {checkpoint_path} not found\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    # Handle different checkpoint formats\n",
    "    if 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    elif 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "    elif 'model' in checkpoint:\n",
    "        state_dict = checkpoint['model']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    # First, explore the structure\n",
    "    all_keys = explore_state_dict(state_dict)\n",
    "    \n",
    "    print(\"Extracting weights...\")\n",
    "    \n",
    "    output_lines = []\n",
    "    output_lines.append(\"#ifndef WEIGHTS_COMBINED_H\")\n",
    "    output_lines.append(\"#define WEIGHTS_COMBINED_H\")\n",
    "    output_lines.append(\"\")\n",
    "    output_lines.append(\"#include \\\"config.h\\\"\")\n",
    "    output_lines.append(\"\")\n",
    "    output_lines.append(\"// Auto-generated weight file from squeeze_qat.pth\")\n",
    "    output_lines.append(\"// Quantized to 8-bit fixed-point (4 int, 4 frac)\")\n",
    "    output_lines.append(\"\")\n",
    "    \n",
    "    # Try to identify architecture from keys\n",
    "    # Common patterns: features.X, conv1, classifier, etc.\n",
    "    \n",
    "    # Conv1 - try multiple possible patterns\n",
    "    print(\"\\nExtracting Conv1 weights...\")\n",
    "    conv1_patterns = ['features.0', 'conv1', 'features.conv1', 'backbone.0']\n",
    "    conv1 = extract_conv_weights(state_dict, conv1_patterns, \"conv1_weights_flat\")\n",
    "    if conv1:\n",
    "        output_lines.append(\"// Conv1: 7x7x3x96\")\n",
    "        output_lines.append(\"inline \" + conv1)\n",
    "        output_lines.append(\"\")\n",
    "    else:\n",
    "        print(\"ERROR: Could not find Conv1 weights!\")\n",
    "        print(\"Please manually check the state_dict structure above.\")\n",
    "    \n",
    "    # Fire modules - try to auto-detect\n",
    "    print(\"\\nExtracting Fire module weights...\")\n",
    "    \n",
    "    # Try to find fire module keys\n",
    "    fire_keys = [k for k in all_keys if 'fire' in k.lower() and 'squeeze' in k.lower()]\n",
    "    \n",
    "    if fire_keys:\n",
    "        print(f\"Found {len(fire_keys)} fire module layers\")\n",
    "        # Extract fire numbers from keys\n",
    "        fire_nums = set()\n",
    "        import re\n",
    "        for key in fire_keys:\n",
    "            match = re.search(r'fire(\\d+)', key.lower())\n",
    "            if match:\n",
    "                fire_nums.add(int(match.group(1)))\n",
    "        \n",
    "        fire_nums = sorted(fire_nums)\n",
    "        print(f\"Fire module numbers detected: {fire_nums}\")\n",
    "        \n",
    "        for fire_num in fire_nums:\n",
    "            print(f\"\\nExtracting Fire{fire_num} weights...\")\n",
    "            # Try multiple naming patterns\n",
    "            fire_patterns = [\n",
    "                f'features.fire{fire_num}',\n",
    "                f'fire{fire_num}',\n",
    "                f'features.{fire_num}',\n",
    "                f'backbone.fire{fire_num}'\n",
    "            ]\n",
    "            fire_weights = extract_fire_weights(state_dict, fire_patterns)\n",
    "            if fire_weights:\n",
    "                output_lines.append(f\"// Fire{fire_num}\")\n",
    "                for weight_def in fire_weights:\n",
    "                    output_lines.append(\"inline \" + weight_def)\n",
    "                output_lines.append(\"\")\n",
    "    else:\n",
    "        print(\"No fire modules found with pattern 'fire' in keys\")\n",
    "        print(\"Attempting fallback: checking features.X for Fire-like structure...\")\n",
    "        \n",
    "        # Fallback: manually specify typical SqueezeNet structure\n",
    "        # features.3, 4, 5, 7, 8, 9, 10, 12 are usually Fire modules\n",
    "        fire_mapping = {\n",
    "            2: ['features.3', 'features.fire2'],\n",
    "            3: ['features.4', 'features.fire3'],\n",
    "            4: ['features.5', 'features.fire4'],\n",
    "            5: ['features.7', 'features.fire5'],\n",
    "            6: ['features.8', 'features.fire6'],\n",
    "            7: ['features.9', 'features.fire7'],\n",
    "            8: ['features.10', 'features.fire8'],\n",
    "            9: ['features.12', 'features.fire9']\n",
    "        }\n",
    "        \n",
    "        for fire_num, patterns in fire_mapping.items():\n",
    "            print(f\"\\nExtracting Fire{fire_num} weights...\")\n",
    "            fire_weights = extract_fire_weights(state_dict, patterns)\n",
    "            if fire_weights:\n",
    "                output_lines.append(f\"// Fire{fire_num}\")\n",
    "                for weight_def in fire_weights:\n",
    "                    output_lines.append(\"inline \" + weight_def)\n",
    "                output_lines.append(\"\")\n",
    "    \n",
    "    # Conv10 (final classifier conv)\n",
    "    print(\"\\nExtracting Conv10 weights...\")\n",
    "    conv10_patterns = ['classifier.1', 'classifier.conv', 'conv10', 'features.13', 'final_conv']\n",
    "    conv10 = extract_conv_weights(state_dict, conv10_patterns, \"conv10_weights_flat\")\n",
    "    if conv10:\n",
    "        output_lines.append(\"// Conv10: 1x1x512x10\")\n",
    "        output_lines.append(\"inline \" + conv10)\n",
    "        output_lines.append(\"\")\n",
    "    else:\n",
    "        print(\"WARNING: Could not find Conv10 weights!\")\n",
    "    \n",
    "    # Close the header guard\n",
    "    output_lines.append(\"#endif // WEIGHTS_COMBINED_H\")\n",
    "    \n",
    "    # Write to file\n",
    "    output_file = \"weights.h\"\n",
    "    print(f\"\\nWriting to {output_file}...\")\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('\\n'.join(output_lines))\n",
    "    \n",
    "    print(f\"✓ Successfully extracted weights to {output_file}\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Compile the project: make weight_load\")\n",
    "    print(\"2. Run the test: ./tb_weight_loading\")\n",
    "    print(\"3. Verify weight values match your expected quantization\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
