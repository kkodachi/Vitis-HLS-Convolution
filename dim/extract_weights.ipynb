{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c11e97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from squeezenet_qat.pth...\n",
      "Loaded 52 parameters\n",
      "\n",
      "======================================================================\n",
      "WEIGHT DISTRIBUTION ANALYSIS\n",
      "======================================================================\n",
      "conv1.weight                             min=-0.2870 max= 0.3948\n",
      "fire2.conv1.weight                       min=-0.6087 max= 0.4776\n",
      "fire2.conv2.weight                       min=-0.1245 max= 0.2498\n",
      "fire2.conv3.weight                       min=-0.2449 max= 0.2877\n",
      "fire3.conv1.weight                       min=-0.5484 max= 0.5835\n",
      "fire3.conv2.weight                       min=-0.1264 max= 0.2192\n",
      "fire3.conv3.weight                       min=-0.2986 max= 0.3525\n",
      "fire4.conv1.weight                       min=-0.7804 max= 0.7987\n",
      "fire4.conv2.weight                       min=-0.0852 max= 0.2099\n",
      "fire4.conv3.weight                       min=-0.2670 max= 0.4170\n",
      "fire5.conv1.weight                       min=-0.4005 max= 0.5334\n",
      "fire5.conv2.weight                       min=-0.2113 max= 0.3256\n",
      "fire5.conv3.weight                       min=-0.2719 max= 0.3134\n",
      "fire6.conv1.weight                       min=-0.3783 max= 0.6264\n",
      "fire6.conv2.weight                       min=-0.1837 max= 0.3461\n",
      "fire6.conv3.weight                       min=-0.3748 max= 0.3671\n",
      "fire7.conv1.weight                       min=-0.4410 max= 0.7181\n",
      "fire7.conv2.weight                       min=-0.1988 max= 0.2434\n",
      "fire7.conv3.weight                       min=-0.3448 max= 0.3184\n",
      "fire8.conv1.weight                       min=-0.3187 max= 0.4149\n",
      "fire8.conv2.weight                       min=-0.0928 max= 0.1378\n",
      "fire8.conv3.weight                       min=-0.2151 max= 0.3110\n",
      "fire9.conv1.weight                       min=-0.2740 max= 0.3553\n",
      "fire9.conv2.weight                       min=-0.1495 max= 0.2090\n",
      "fire9.conv3.weight                       min=-0.2719 max= 0.3055\n",
      "conv10.weight                            min=-0.3093 max= 0.6625\n",
      "\n",
      "======================================================================\n",
      "OVERALL: min=-0.7804, max=0.7987\n",
      "         mean=-0.0007, std=0.0242\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "EXTRACTING WEIGHTS\n",
      "======================================================================\n",
      "\n",
      "[1/11] Extracting Conv1...\n",
      "  Found: conv1.weight\n",
      "  Shape: (96, 3, 7, 7)\n",
      "  Input range: [-0.2870, 0.3948]\n",
      "  Output int range: [-5, 6]\n",
      "  Represents float: [-0.3125, 0.3750]\n",
      "\n",
      "[2/11] Extracting Fire2...\n",
      "  Squeeze (conv1): torch.Size([16, 96, 1, 1])\n",
      "  Expand1 (conv2): torch.Size([64, 16, 1, 1])\n",
      "  Expand3 (conv3): torch.Size([64, 16, 3, 3])\n",
      "\n",
      "[3/11] Extracting Fire3...\n",
      "  Squeeze (conv1): torch.Size([16, 128, 1, 1])\n",
      "  Expand1 (conv2): torch.Size([64, 16, 1, 1])\n",
      "  Expand3 (conv3): torch.Size([64, 16, 3, 3])\n",
      "\n",
      "[4/11] Extracting Fire4...\n",
      "  Squeeze (conv1): torch.Size([32, 128, 1, 1])\n",
      "  Expand1 (conv2): torch.Size([128, 32, 1, 1])\n",
      "  Expand3 (conv3): torch.Size([128, 32, 3, 3])\n",
      "\n",
      "[5/11] Extracting Fire5...\n",
      "  Squeeze (conv1): torch.Size([32, 256, 1, 1])\n",
      "  Expand1 (conv2): torch.Size([128, 32, 1, 1])\n",
      "  Expand3 (conv3): torch.Size([128, 32, 3, 3])\n",
      "\n",
      "[6/11] Extracting Fire6...\n",
      "  Squeeze (conv1): torch.Size([48, 256, 1, 1])\n",
      "  Expand1 (conv2): torch.Size([192, 48, 1, 1])\n",
      "  Expand3 (conv3): torch.Size([192, 48, 3, 3])\n",
      "\n",
      "[7/11] Extracting Fire7...\n",
      "  Squeeze (conv1): torch.Size([48, 384, 1, 1])\n",
      "  Expand1 (conv2): torch.Size([192, 48, 1, 1])\n",
      "  Expand3 (conv3): torch.Size([192, 48, 3, 3])\n",
      "\n",
      "[8/11] Extracting Fire8...\n",
      "  Squeeze (conv1): torch.Size([64, 384, 1, 1])\n",
      "  Expand1 (conv2): torch.Size([256, 64, 1, 1])\n",
      "  Expand3 (conv3): torch.Size([256, 64, 3, 3])\n",
      "\n",
      "[9/11] Extracting Fire9...\n",
      "  Squeeze (conv1): torch.Size([64, 512, 1, 1])\n",
      "  Expand1 (conv2): torch.Size([256, 64, 1, 1])\n",
      "  Expand3 (conv3): torch.Size([256, 64, 3, 3])\n",
      "\n",
      "[11/11] Extracting Conv10...\n",
      "  Found: conv10.weight\n",
      "  Shape: (10, 512, 1, 1)\n",
      "  Input range: [-0.3093, 0.6625]\n",
      "  Output int range: [-5, 11]\n",
      "  Represents float: [-0.3125, 0.6875]\n",
      "\n",
      "======================================================================\n",
      "Writing to weights_combined.h...\n",
      "✓ Successfully extracted 737,568 weights\n",
      "✓ File size: 2.38 MB\n",
      "======================================================================\n",
      "\n",
      "Next steps:\n",
      "1. Compile: make clean && make weight_load\n",
      "2. Test: ./tb_weight_loading\n",
      "3. Verify weight ranges are within [-128, 127]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Extract quantized weights from PyTorch checkpoint and generate C++ header file.\n",
    "Fixed for checkpoint with fire.conv1/conv2/conv3 naming convention.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_weight_distribution(state_dict):\n",
    "    \"\"\"Analyze weight ranges across all layers\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"WEIGHT DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    all_weights = []\n",
    "    for key, val in state_dict.items():\n",
    "        if 'weight' in key:\n",
    "            weights = val.cpu().numpy()\n",
    "            all_weights.append(weights.flatten())\n",
    "            print(f\"{key:40s} min={weights.min():7.4f} max={weights.max():7.4f}\")\n",
    "    \n",
    "    all_weights = np.concatenate(all_weights)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"OVERALL: min={all_weights.min():.4f}, max={all_weights.max():.4f}\")\n",
    "    print(f\"         mean={all_weights.mean():.4f}, std={all_weights.std():.4f}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return all_weights.min(), all_weights.max()\n",
    "\n",
    "def quantize_to_fixed_point(weight, int_bits=4, frac_bits=4, verbose=False):\n",
    "    \"\"\"\n",
    "    Convert float weights to fixed-point representation.\n",
    "    For <8,4>: range is -8.0 to +7.9375\n",
    "    \n",
    "    If weights exceed this range, we'll scale them down.\n",
    "    \"\"\"\n",
    "    # Check input range\n",
    "    w_min, w_max = weight.min(), weight.max()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Input range: [{w_min:.4f}, {w_max:.4f}]\")\n",
    "    \n",
    "    # Target range for signed fixed-point\n",
    "    max_val = (2 ** (int_bits - 1)) - 1  # 7 for 4 int bits\n",
    "    min_val = -(2 ** (int_bits - 1))      # -8 for 4 int bits\n",
    "    max_fp = max_val + (1 - 2**(-frac_bits))  # 7.9375\n",
    "    min_fp = min_val  # -8.0\n",
    "    \n",
    "    # If weights exceed range, apply scaling\n",
    "    scale_factor = 1.0\n",
    "    if w_max > max_fp or w_min < min_fp:\n",
    "        # Calculate required scale to fit within range\n",
    "        scale_down = max(abs(w_max / max_fp), abs(w_min / min_fp))\n",
    "        scale_factor = 1.0 / scale_down\n",
    "        print(f\"  WARNING: Weights [{w_min:.4f}, {w_max:.4f}] exceed [{min_fp}, {max_fp}]\")\n",
    "        print(f\"  Scaling by {scale_factor:.6f} to fit range\")\n",
    "        weight = weight * scale_factor\n",
    "    \n",
    "    # Scale to fixed-point\n",
    "    scale = 2 ** frac_bits  # 16 for 4 fractional bits\n",
    "    \n",
    "    # Quantize: multiply by scale and round\n",
    "    quantized = np.round(weight * scale).astype(np.int32)\n",
    "    \n",
    "    # Saturate to valid range (should rarely trigger after scaling)\n",
    "    max_int = (2 ** (int_bits + frac_bits - 1)) - 1  # 127 for 8-bit\n",
    "    min_int = -(2 ** (int_bits + frac_bits - 1))     # -128 for 8-bit\n",
    "    \n",
    "    clipped = np.clip(quantized, min_int, max_int)\n",
    "    if np.any(clipped != quantized):\n",
    "        n_clipped = np.sum(clipped != quantized)\n",
    "        print(f\"  Clipped {n_clipped} values to [{min_int}, {max_int}]\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Output int range: [{clipped.min()}, {clipped.max()}]\")\n",
    "        print(f\"  Represents float: [{clipped.min()/scale:.4f}, {clipped.max()/scale:.4f}]\")\n",
    "    \n",
    "    return clipped\n",
    "\n",
    "def format_array_cpp(name, data, line_width=12):\n",
    "    \"\"\"Format numpy array as C++ array initialization\"\"\"\n",
    "    flat = data.flatten()\n",
    "    lines = []\n",
    "    lines.append(f\"const fixed_point_t {name}[{len(flat)}] = {{\")\n",
    "    \n",
    "    # Format in rows\n",
    "    for i in range(0, len(flat), line_width):\n",
    "        row = flat[i:i+line_width]\n",
    "        row_str = \", \".join(f\"{int(x)}\" for x in row)\n",
    "        lines.append(f\"    {row_str},\")\n",
    "    \n",
    "    # Remove trailing comma from last line\n",
    "    if lines[-1].endswith(\",\"):\n",
    "        lines[-1] = lines[-1][:-1]\n",
    "    \n",
    "    lines.append(\"};\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def extract_conv_weights(state_dict, key_name, out_name, verbose=False):\n",
    "    \"\"\"Extract convolutional layer weights\"\"\"\n",
    "    if key_name not in state_dict:\n",
    "        print(f\"  ERROR: {key_name} not found!\")\n",
    "        return None\n",
    "    \n",
    "    weight = state_dict[key_name].cpu().numpy()\n",
    "    print(f\"  Found: {key_name}\")\n",
    "    print(f\"  Shape: {weight.shape}\")\n",
    "    quantized = quantize_to_fixed_point(weight, verbose=verbose)\n",
    "    return format_array_cpp(out_name, quantized)\n",
    "\n",
    "def extract_fire_module(state_dict, fire_name, fire_num, verbose=False):\n",
    "    \"\"\"Extract all weights from a Fire module\"\"\"\n",
    "    # Your checkpoint uses: fire2.conv1, fire2.conv2, fire2.conv3\n",
    "    squeeze_key = f\"{fire_name}.conv1.weight\"  # squeeze = conv1\n",
    "    expand1_key = f\"{fire_name}.conv2.weight\"  # expand1x1 = conv2\n",
    "    expand3_key = f\"{fire_name}.conv3.weight\"  # expand3x3 = conv3\n",
    "    \n",
    "    # Check all keys exist\n",
    "    if squeeze_key not in state_dict:\n",
    "        print(f\"  ERROR: {squeeze_key} not found!\")\n",
    "        return None\n",
    "    if expand1_key not in state_dict:\n",
    "        print(f\"  ERROR: {expand1_key} not found!\")\n",
    "        return None\n",
    "    if expand3_key not in state_dict:\n",
    "        print(f\"  ERROR: {expand3_key} not found!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Squeeze (conv1): {state_dict[squeeze_key].shape}\")\n",
    "    print(f\"  Expand1 (conv2): {state_dict[expand1_key].shape}\")\n",
    "    print(f\"  Expand3 (conv3): {state_dict[expand3_key].shape}\")\n",
    "    \n",
    "    # Extract and quantize\n",
    "    squeeze = state_dict[squeeze_key].cpu().numpy()\n",
    "    squeeze_quant = quantize_to_fixed_point(squeeze, verbose=verbose)\n",
    "    \n",
    "    expand1 = state_dict[expand1_key].cpu().numpy()\n",
    "    expand1_quant = quantize_to_fixed_point(expand1, verbose=verbose)\n",
    "    \n",
    "    expand3 = state_dict[expand3_key].cpu().numpy()\n",
    "    expand3_quant = quantize_to_fixed_point(expand3, verbose=verbose)\n",
    "    \n",
    "    results = []\n",
    "    results.append(format_array_cpp(f\"fire{fire_num}_squeeze_weights_flat\", squeeze_quant))\n",
    "    results.append(format_array_cpp(f\"fire{fire_num}_expand1x1_weights_flat\", expand1_quant))\n",
    "    results.append(format_array_cpp(f\"fire{fire_num}_expand3x3_weights_flat\", expand3_quant))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    checkpoint_path = \"squeezenet_qat.pth\"\n",
    "    \n",
    "    if not Path(checkpoint_path).exists():\n",
    "        print(f\"Error: File {checkpoint_path} not found\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    # Handle different checkpoint formats\n",
    "    if 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    elif 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "    elif 'model' in checkpoint:\n",
    "        state_dict = checkpoint['model']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    print(f\"Loaded {len(state_dict)} parameters\")\n",
    "    \n",
    "    # Analyze weight distribution\n",
    "    w_min, w_max = analyze_weight_distribution(state_dict)\n",
    "    \n",
    "    # Check if we need global scaling\n",
    "    target_max = 7.9375\n",
    "    if abs(w_max) > target_max or abs(w_min) > target_max:\n",
    "        print(f\"\\n⚠️  WARNING: Weights range [{w_min:.4f}, {w_max:.4f}]\")\n",
    "        print(f\"   exceeds target range [-8.0, 7.9375]\")\n",
    "        print(f\"   Each layer will be scaled individually to fit.\\n\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXTRACTING WEIGHTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    output_lines = []\n",
    "    output_lines.append(\"#ifndef WEIGHTS_COMBINED_H\")\n",
    "    output_lines.append(\"#define WEIGHTS_COMBINED_H\")\n",
    "    output_lines.append(\"\")\n",
    "    output_lines.append(\"#include \\\"config.h\\\"\")\n",
    "    output_lines.append(\"\")\n",
    "    output_lines.append(\"// Auto-generated weight file from squeezenet_qat.pth\")\n",
    "    output_lines.append(\"// Quantized to 8-bit fixed-point (4 int, 4 frac)\")\n",
    "    output_lines.append(\"// Range: -8.0 to +7.9375 (resolution: 0.0625)\")\n",
    "    output_lines.append(\"\")\n",
    "    \n",
    "    # Conv1\n",
    "    print(\"\\n[1/11] Extracting Conv1...\")\n",
    "    conv1 = extract_conv_weights(state_dict, \"conv1.weight\", \"conv1_weights_flat\", verbose=True)\n",
    "    if conv1:\n",
    "        output_lines.append(\"// Conv1: 7x7x3x96 = 14,112 weights\")\n",
    "        output_lines.append(\"inline \" + conv1)\n",
    "        output_lines.append(\"\")\n",
    "    else:\n",
    "        print(\"ERROR: Conv1 extraction failed!\")\n",
    "        return\n",
    "    \n",
    "    # Fire modules 2-9\n",
    "    fire_modules = [\n",
    "        (2, \"fire2\"),\n",
    "        (3, \"fire3\"),\n",
    "        (4, \"fire4\"),\n",
    "        (5, \"fire5\"),\n",
    "        (6, \"fire6\"),\n",
    "        (7, \"fire7\"),\n",
    "        (8, \"fire8\"),\n",
    "        (9, \"fire9\")\n",
    "    ]\n",
    "    \n",
    "    for idx, (fire_num, fire_name) in enumerate(fire_modules, start=2):\n",
    "        print(f\"\\n[{idx}/11] Extracting Fire{fire_num}...\")\n",
    "        fire_weights = extract_fire_module(state_dict, fire_name, fire_num, verbose=False)\n",
    "        if fire_weights:\n",
    "            output_lines.append(f\"// Fire{fire_num}\")\n",
    "            for weight_def in fire_weights:\n",
    "                output_lines.append(\"inline \" + weight_def)\n",
    "            output_lines.append(\"\")\n",
    "        else:\n",
    "            print(f\"ERROR: Fire{fire_num} extraction failed!\")\n",
    "    \n",
    "    # Conv10\n",
    "    print(\"\\n[11/11] Extracting Conv10...\")\n",
    "    conv10 = extract_conv_weights(state_dict, \"conv10.weight\", \"conv10_weights_flat\", verbose=True)\n",
    "    if conv10:\n",
    "        output_lines.append(\"// Conv10: 1x1x512x10 = 5,120 weights\")\n",
    "        output_lines.append(\"inline \" + conv10)\n",
    "        output_lines.append(\"\")\n",
    "    else:\n",
    "        print(\"ERROR: Conv10 extraction failed!\")\n",
    "        return\n",
    "    \n",
    "    # Close header guard\n",
    "    output_lines.append(\"#endif // WEIGHTS_COMBINED_H\")\n",
    "    \n",
    "    # Write to file\n",
    "    output_file = \"weights_combined.h\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Writing to {output_file}...\")\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('\\n'.join(output_lines))\n",
    "    \n",
    "    # Calculate total weights\n",
    "    total_weights = (\n",
    "        14112 +      # conv1\n",
    "        1536 + 1024 + 9216 +   # fire2\n",
    "        2048 + 1024 + 9216 +   # fire3\n",
    "        4096 + 4096 + 36864 +  # fire4\n",
    "        8192 + 4096 + 36864 +  # fire5\n",
    "        12288 + 9216 + 82944 + # fire6\n",
    "        18432 + 9216 + 82944 + # fire7\n",
    "        24576 + 16384 + 147456 + # fire8\n",
    "        32768 + 16384 + 147456 + # fire9\n",
    "        5120         # conv10\n",
    "    )\n",
    "    \n",
    "    file_size = Path(output_file).stat().st_size / (1024 * 1024)\n",
    "    \n",
    "    print(f\"✓ Successfully extracted {total_weights:,} weights\")\n",
    "    print(f\"✓ File size: {file_size:.2f} MB\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    print(\"Next steps:\")\n",
    "    print(\"1. Compile: make clean && make weight_load\")\n",
    "    print(\"2. Test: ./tb_weight_loading\")\n",
    "    print(\"3. Verify weight ranges are within [-128, 127]\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
