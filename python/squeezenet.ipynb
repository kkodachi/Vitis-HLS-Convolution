{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04e8316b",
   "metadata": {},
   "source": [
    "# SqueezeNet\n",
    "\n",
    "In this file we train the SqueezeNet model as described in the paper found [here](https://arxiv.org/abs/1602.07360).\n",
    "This implementation uses the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.ao.quantization import QuantStub, DeQuantStub\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a33ed",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "The class in this cell below defines our architecture and defines our forward pass. We insert quantization stub for later Quantization Aware Training. We also define helper functions to save and load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ae5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Fire(nn.Module):\n",
    "    def __init__(self, inplanes, squeeze_planes, expand_planes):\n",
    "        super(Fire, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
    "        # self.bn1 = nn.BatchNorm2d(squeeze_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=1)\n",
    "        # self.bn2 = nn.BatchNorm2d(expand_planes)\n",
    "        self.conv3 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=3, padding=1)\n",
    "        # self.bn3 = nn.BatchNorm2d(expand_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # MSR initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        # out1 = self.bn2(self.conv2(x))\n",
    "        # out2 = self.bn3(self.conv3(x))\n",
    "        # out = torch.cat([out1, out2], 1)\n",
    "        # out = self.relu2(out)\n",
    "        # return out\n",
    "\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        out1 = self.conv2(x)\n",
    "        out2 = self.conv3(x)\n",
    "        out = torch.cat([out1, out2], 1)\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "\n",
    "class SqueezeNetCIFAR10(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SqueezeNetCIFAR10, self).__init__()\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(96)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool1 = nn.MaxPool2d(2, 2)  # 32 -> 16\n",
    "\n",
    "        self.fire2 = Fire(96, 16, 64)\n",
    "        self.fire3 = Fire(128, 16, 64)\n",
    "        self.fire4 = Fire(128, 32, 128)\n",
    "        self.maxpool2 = nn.MaxPool2d(2, 2)  # 16 -> 8\n",
    "\n",
    "        self.fire5 = Fire(256, 32, 128)\n",
    "        self.fire6 = Fire(256, 48, 192)\n",
    "        self.fire7 = Fire(384, 48, 192)\n",
    "        self.fire8 = Fire(384, 64, 256)\n",
    "        self.maxpool3 = nn.MaxPool2d(2, 2)  # 8 -> 4\n",
    "\n",
    "        self.fire9 = Fire(512, 64, 256)\n",
    "        self.conv10 = nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "        self.avg_pool = nn.AvgPool2d(4)\n",
    "        # self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.fire2(x)\n",
    "        x = self.fire3(x)\n",
    "        x = self.fire4(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.fire5(x)\n",
    "        x = self.fire6(x)\n",
    "        x = self.fire7(x)\n",
    "        x = self.fire8(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = self.fire9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dequant(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def load_model(self, path='squeezenet_cifar10.pth',device='cpu'):\n",
    "        state_dict = torch.load(path,map_location=device)\n",
    "\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            if k.startswith('module.'):\n",
    "                k = k[len('module.'):]\n",
    "            new_state_dict[k] = v\n",
    "\n",
    "        self.load_state_dict(new_state_dict)\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "\n",
    "        print(f\"Model loaded from {path}\")\n",
    "        # print(self)\n",
    "\n",
    "    def save_model(self, path='squeezenet_cifar10.pth'):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        print(f\"Model saved to {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6a2464",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "In this cell we define a function to load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dadba32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path='./data', batch_size=64):\n",
    "  print(\"Loading the CIFAR10 dataset\")\n",
    "\n",
    "  transform = transforms.Compose([\n",
    "      transforms.ToTensor(), # scale RGB 0-255 to 0-1\n",
    "      # normalize with known mean and std deviation of the CIFAR10 dataset\n",
    "      transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))\n",
    "      ])\n",
    "\n",
    "  # get training data\n",
    "  train_dataset = datasets.CIFAR10(root=path, train=True, download=True, transform=transform)\n",
    "  # get test data\n",
    "  test_dataset = datasets.CIFAR10(root=path, train=False, download=True, transform=transform)\n",
    "  # load the training data\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "  # load the test data\n",
    "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "  print(f\"Loaded train data: {len(train_loader.dataset)} total samples, {len(train_loader)} batches\\n\"\n",
    "      f\"Loaded test data: {len(test_loader.dataset)} total samples, {len(test_loader)} batches\")\n",
    "\n",
    "  return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2d80055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the CIFAR10 dataset\n",
      "Loaded train data: 50000 total samples, 782 batches\n",
      "Loaded test data: 10000 total samples, 157 batches\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb673199",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "In the cells below we define a function to visualize our training and train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82cd0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(metrics):\n",
    "  train_losses = metrics.get('train_loss',None)\n",
    "  test_losses = metrics.get('test_loss',None)\n",
    "  train_accs = metrics.get('train_acc',None)\n",
    "  test_accs = metrics.get('test_acc',None)\n",
    "\n",
    "  epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "  plt.figure(figsize=(12, 5))\n",
    "\n",
    "  # Loss Graph\n",
    "  plt.subplot(1, 2, 1)\n",
    "  if train_losses:\n",
    "    plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
    "  if test_losses:\n",
    "    plt.plot(epochs, test_losses, label='Test Loss', marker='s')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.title('Training vs Test Loss')\n",
    "  plt.legend()\n",
    "  plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "  # Accuracy Graph\n",
    "  plt.subplot(1, 2, 2)\n",
    "  if train_accs:\n",
    "    plt.plot(epochs, train_accs, label='Train Accuracy', marker='o')\n",
    "  if test_accs:\n",
    "    plt.plot(epochs, test_accs, label='Test Accuracy', marker='s')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Accuracy (%)')\n",
    "  plt.title('Training vs Test Accuracy')\n",
    "  plt.legend()\n",
    "  plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c9f93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_loader,test_loader,train=True,test=True,device='cpu',epochs=10,lr=1e-3):\n",
    "  model.to(device)\n",
    "  metrics = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "  # TRAINING LOOP\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "  for e in range(epochs):\n",
    "    print(f\"Epoch [{e+1}/{epochs}] \",end='')\n",
    "    if train:\n",
    "      model.train()\n",
    "      train_loss, total_examples, correct = 0.0, 0, 0\n",
    "\n",
    "      for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad() # zero gradients\n",
    "        outputs = model(inputs) # forward pass\n",
    "        loss = criterion(outputs,labels) # get loss from cost function\n",
    "        loss.backward() # backward propagation\n",
    "        optimizer.step() # update gradients\n",
    "\n",
    "        # train_loss += loss.item() # track total loss up to this point\n",
    "        train_loss += loss.item() * labels.size(0)\n",
    "        _, pred_ind = outputs.max(1) # get index of prediction (highest value)\n",
    "        total_examples += labels.size(0) # update count for this epoch with batch size\n",
    "        correct += pred_ind.eq(labels).sum().item() # return count of correct predictions\n",
    "\n",
    "    #   train_loss /= len(train_loader) # get average per batch\n",
    "      train_loss /= total_examples # get average per example\n",
    "      train_acc = 100.0 * correct / total_examples\n",
    "\n",
    "      metrics[\"train_loss\"].append(train_loss)\n",
    "      metrics[\"train_acc\"].append(train_acc)\n",
    "\n",
    "      print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% \",end='')\n",
    "\n",
    "      # VALIDATION/TEST\n",
    "    if test:\n",
    "      model.eval()\n",
    "      test_loss, total_examples, correct = 0.0, 0, 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "          outputs = model(inputs) # forward pass\n",
    "          loss = criterion(outputs,labels) # get loss from cost function\n",
    "          test_loss += loss.item() # update loss\n",
    "          _, pred_ind = outputs.max(1) # get index of prediction (highest value)\n",
    "          total_examples += labels.size(0) # update count for this epoch with batch size\n",
    "          correct += pred_ind.eq(labels).sum().item() # return count of correct predictions\n",
    "\n",
    "      test_loss /= len(test_loader)\n",
    "      test_acc = 100.0 * correct / total_examples\n",
    "\n",
    "      metrics[\"test_loss\"].append(test_loss)\n",
    "      metrics[\"test_acc\"].append(test_acc)\n",
    "\n",
    "      print(f\"Test/Val Loss: {test_loss:.4f}, Test/Val Acc: {test_acc:.2f}%\")\n",
    "\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de683d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device=cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device={device}\")\n",
    "\n",
    "model_fp32 = SqueezeNetCIFAR10()\n",
    "\n",
    "# model_fp32.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33fcd637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 1.7124, Train Acc: 34.29% Test/Val Loss: 1.4589, Test/Val Acc: 44.90%\n",
      "Epoch [2/100] Train Loss: 1.3551, Train Acc: 49.81% Test/Val Loss: 1.2528, Test/Val Acc: 54.69%\n",
      "Epoch [3/100] Train Loss: 1.1891, Train Acc: 56.70% Test/Val Loss: 1.1352, Test/Val Acc: 59.17%\n",
      "Epoch [4/100] Train Loss: 1.0623, Train Acc: 61.84% Test/Val Loss: 1.0366, Test/Val Acc: 63.34%\n",
      "Epoch [5/100] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m train, test = \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      2\u001b[39m epochs = \u001b[32m100\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m fp32_metrics = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_fp32\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, test_loader, train, test, device, epochs, lr)\u001b[39m\n\u001b[32m     26\u001b[39m optimizer.step() \u001b[38;5;66;03m# update gradients\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# train_loss += loss.item() # track total loss up to this point\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * labels.size(\u001b[32m0\u001b[39m)\n\u001b[32m     30\u001b[39m _, pred_ind = outputs.max(\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# get index of prediction (highest value)\u001b[39;00m\n\u001b[32m     31\u001b[39m total_examples += labels.size(\u001b[32m0\u001b[39m) \u001b[38;5;66;03m# update count for this epoch with batch size\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train, test = True, True\n",
    "epochs = 100\n",
    "fp32_metrics = train_model(model=model_fp32,train_loader=train_loader,test_loader=test_loader,train=train,test=test,device=device,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb6d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32.save_model(\"squeezenet_cifar10_fp32.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(fp32_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
