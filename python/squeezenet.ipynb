{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04e8316b",
   "metadata": {},
   "source": [
    "# SqueezeNet\n",
    "\n",
    "In this file we train the SqueezeNet model as described in the paper found [here](https://arxiv.org/abs/1602.07360).\n",
    "This implementation uses the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.ao.quantization import QuantStub, DeQuantStub\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a33ed",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "The class in this cell below defines our architecture and defines our forward pass. We insert quantization stub for later Quantization Aware Training. We also define helper functions to save and load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430038d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, input_channels=3):\n",
    "        super(SqueezeNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # quantization stubs (identity during FP32 training)\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "        # conv1\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            input_channels, 96, kernel_size=7, stride=2, padding=3\n",
    "        )\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        # fire modules\n",
    "        self.fire2_squeeze = nn.Conv2d(96, 16, kernel_size=1)\n",
    "        self.fire2_expand1x1 = nn.Conv2d(16, 64, kernel_size=1)\n",
    "        self.fire2_expand3x3 = nn.Conv2d(16, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fire3_squeeze = nn.Conv2d(128, 16, kernel_size=1)\n",
    "        self.fire3_expand1x1 = nn.Conv2d(16, 64, kernel_size=1)\n",
    "        self.fire3_expand3x3 = nn.Conv2d(16, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fire4_squeeze = nn.Conv2d(128, 32, kernel_size=1)\n",
    "        self.fire4_expand1x1 = nn.Conv2d(32, 128, kernel_size=1)\n",
    "        self.fire4_expand3x3 = nn.Conv2d(32, 128, kernel_size=3, padding=1)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.fire5_squeeze = nn.Conv2d(256, 32, kernel_size=1)\n",
    "        self.fire5_expand1x1 = nn.Conv2d(32, 128, kernel_size=1)\n",
    "        self.fire5_expand3x3 = nn.Conv2d(32, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fire6_squeeze = nn.Conv2d(256, 48, kernel_size=1)\n",
    "        self.fire6_expand1x1 = nn.Conv2d(48, 192, kernel_size=1)\n",
    "        self.fire6_expand3x3 = nn.Conv2d(48, 192, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fire7_squeeze = nn.Conv2d(384, 48, kernel_size=1)\n",
    "        self.fire7_expand1x1 = nn.Conv2d(48, 192, kernel_size=1)\n",
    "        self.fire7_expand3x3 = nn.Conv2d(48, 192, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fire8_squeeze = nn.Conv2d(384, 64, kernel_size=1)\n",
    "        self.fire8_expand1x1 = nn.Conv2d(64, 256, kernel_size=1)\n",
    "        self.fire8_expand3x3 = nn.Conv2d(64, 256, kernel_size=3, padding=1)\n",
    "        self.maxpool8 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.fire9_squeeze = nn.Conv2d(512, 64, kernel_size=1)\n",
    "        self.fire9_expand1x1 = nn.Conv2d(64, 256, kernel_size=1)\n",
    "        self.fire9_expand3x3 = nn.Conv2d(64, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # conv10\n",
    "        self.conv10 = nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "\n",
    "    def _fire_forward(self, x, squeeze, expand1, expand3):\n",
    "        x = self.relu(squeeze(x))\n",
    "        e1 = self.relu(expand1(x))\n",
    "        e3 = self.relu(expand3(x))\n",
    "        return torch.cat([e1, e3], dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # quant input (for task 2)\n",
    "        x = self.quant(x)\n",
    "\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self._fire_forward(x, self.fire2_squeeze, self.fire2_expand1x1, self.fire2_expand3x3)\n",
    "        x = self._fire_forward(x, self.fire3_squeeze, self.fire3_expand1x1, self.fire3_expand3x3)\n",
    "\n",
    "        x = self._fire_forward(x, self.fire4_squeeze, self.fire4_expand1x1, self.fire4_expand3x3)\n",
    "        x = self.maxpool4(x)\n",
    "\n",
    "        x = self._fire_forward(x, self.fire5_squeeze, self.fire5_expand1x1, self.fire5_expand3x3)\n",
    "        x = self._fire_forward(x, self.fire6_squeeze, self.fire6_expand1x1, self.fire6_expand3x3)\n",
    "        x = self._fire_forward(x, self.fire7_squeeze, self.fire7_expand1x1, self.fire7_expand3x3)\n",
    "\n",
    "        x = self._fire_forward(x, self.fire8_squeeze, self.fire8_expand1x1, self.fire8_expand3x3)\n",
    "        x = self.maxpool8(x)\n",
    "\n",
    "        x = self._fire_forward(x, self.fire9_squeeze, self.fire9_expand1x1, self.fire9_expand3x3)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv10(x)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # dequant before logits output (for task 2)\n",
    "        x = self.dequant(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def load_model(self, path='squeezenet_cifar10.pth',device='cpu'):\n",
    "        state_dict = torch.load(path,map_location=device)\n",
    "\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            if k.startswith('module.'):\n",
    "                k = k[len('module.'):]\n",
    "            new_state_dict[k] = v\n",
    "\n",
    "        self.load_state_dict(new_state_dict)\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "\n",
    "        print(f\"Model loaded from {path}\")\n",
    "        # print(self)\n",
    "\n",
    "    def save_model(self, path='squeezenet_cifar10.pth'):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        print(f\"Model saved to {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6a2464",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "In this cell we define a function to load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadba32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path='./data', batch_size=64):\n",
    "  print(\"Loading the CIFAR10 dataset\")\n",
    "\n",
    "  transform = transforms.Compose([\n",
    "      transforms.ToTensor(), # scale RGB 0-255 to 0-1\n",
    "      # normalize with known mean and std deviation of the CIFAR10 dataset\n",
    "      transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))\n",
    "      ])\n",
    "\n",
    "  # get training data\n",
    "  train_dataset = datasets.CIFAR10(root=path, train=True, download=True, transform=transform)\n",
    "  # get test data\n",
    "  test_dataset = datasets.CIFAR10(root=path, train=False, download=True, transform=transform)\n",
    "  # load the training data\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "  # load the test data\n",
    "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "  print(f\"Loaded train data: {len(train_loader.dataset)} total samples, {len(train_loader)} batches\\n\"\n",
    "      f\"Loaded test data: {len(test_loader.dataset)} total samples, {len(test_loader)} batches\")\n",
    "\n",
    "  return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d80055",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb673199",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "In the cells below we define a function to visualize our training and train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82cd0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(metrics):\n",
    "  train_losses = metrics.get('train_loss',None)\n",
    "  test_losses = metrics.get('test_loss',None)\n",
    "  train_accs = metrics.get('train_acc',None)\n",
    "  test_accs = metrics.get('test_acc',None)\n",
    "\n",
    "  epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "  plt.figure(figsize=(12, 5))\n",
    "\n",
    "  # Loss Graph\n",
    "  plt.subplot(1, 2, 1)\n",
    "  if train_losses:\n",
    "    plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
    "  if test_losses:\n",
    "    plt.plot(epochs, test_losses, label='Test Loss', marker='s')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.title('Training vs Test Loss')\n",
    "  plt.legend()\n",
    "  plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "  # Accuracy Graph\n",
    "  plt.subplot(1, 2, 2)\n",
    "  if train_accs:\n",
    "    plt.plot(epochs, train_accs, label='Train Accuracy', marker='o')\n",
    "  if test_accs:\n",
    "    plt.plot(epochs, test_accs, label='Test Accuracy', marker='s')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Accuracy (%)')\n",
    "  plt.title('Training vs Test Accuracy')\n",
    "  plt.legend()\n",
    "  plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_loader,test_loader,train=True,test=True,device='cpu',epochs=10,lr=1e-3):\n",
    "  model.to(device)\n",
    "  metrics = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "  # TRAINING LOOP\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "  for e in range(epochs):\n",
    "    print(f\"Epoch [{e+1}/{epochs}] \",end='')\n",
    "    if train:\n",
    "      model.train()\n",
    "      train_loss, total_examples, correct = 0.0, 0, 0\n",
    "\n",
    "      for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad() # zero gradients\n",
    "        outputs = model(inputs) # forward pass\n",
    "        loss = criterion(outputs,labels) # get loss from cost function\n",
    "        loss.backward() # backward propagation\n",
    "        optimizer.step() # update gradients\n",
    "\n",
    "        # train_loss += loss.item() # track total loss up to this point\n",
    "        train_loss += loss.item() * labels.size(0)\n",
    "        _, pred_ind = outputs.max(1) # get index of prediction (highest value)\n",
    "        total_examples += labels.size(0) # update count for this epoch with batch size\n",
    "        correct += pred_ind.eq(labels).sum().item() # return count of correct predictions\n",
    "\n",
    "    #   train_loss /= len(train_loader) # get average per batch\n",
    "      train_loss /= total_examples # get average per example\n",
    "      train_acc = 100.0 * correct / total_examples\n",
    "\n",
    "      metrics[\"train_loss\"].append(train_loss)\n",
    "      metrics[\"train_acc\"].append(train_acc)\n",
    "\n",
    "      print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% \",end='')\n",
    "\n",
    "      # VALIDATION/TEST\n",
    "    if test:\n",
    "      model.eval()\n",
    "      test_loss, total_examples, correct = 0.0, 0, 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "          outputs = model(inputs) # forward pass\n",
    "          loss = criterion(outputs,labels) # get loss from cost function\n",
    "          test_loss += loss.item() # update loss\n",
    "          _, pred_ind = outputs.max(1) # get index of prediction (highest value)\n",
    "          total_examples += labels.size(0) # update count for this epoch with batch size\n",
    "          correct += pred_ind.eq(labels).sum().item() # return count of correct predictions\n",
    "\n",
    "      test_loss /= len(test_loader)\n",
    "      test_acc = 100.0 * correct / total_examples\n",
    "\n",
    "      metrics[\"test_loss\"].append(test_loss)\n",
    "      metrics[\"test_acc\"].append(test_acc)\n",
    "\n",
    "      print(f\"Test/Val Loss: {test_loss:.4f}, Test/Val Acc: {test_acc:.2f}%\")\n",
    "\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de683d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device={device}\")\n",
    "\n",
    "model_fp32 = SqueezeNet()\n",
    "\n",
    "# model_fp32.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fcd637",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = True, True\n",
    "epochs = 100\n",
    "fp32_metrics = train_model(model=model_fp32,train_loader=train_loader,test_loader=test_loader,train=train,test=test,device=device,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb6d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32.save_model(\"squeezenet_cifar10_fp32.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(fp32_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
