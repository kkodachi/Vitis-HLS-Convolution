{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526c0690",
   "metadata": {},
   "source": [
    "# EE511 Final Project\n",
    "\n",
    "In this file we train the SqueezeNet model as described in the paper found [here](https://arxiv.org/abs/1602.07360).\n",
    "This implementation uses the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e8316b",
   "metadata": {},
   "source": [
    "## Task 1: Train SqueezeNet\n",
    "\n",
    "For task 1 we train SqueezeNet for 100 epochs and are able to get a final test accuracy of 69.66%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "004e9853",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T21:07:31.340167Z",
     "iopub.status.busy": "2025-12-07T21:07:31.339685Z",
     "iopub.status.idle": "2025-12-07T21:07:41.754919Z",
     "shell.execute_reply": "2025-12-07T21:07:41.754368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device=cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.ao.quantization import QuantStub, DeQuantStub\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device={device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a33ed",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "The class in this cell below defines our architecture and defines our forward pass. We insert quantization stub for later Quantization Aware Training. We also define helper functions to save and load the model.\n",
    "\n",
    "Note: MSR Initialization was added because the training would not work without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7ae5ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T21:07:41.760192Z",
     "iopub.status.busy": "2025-12-07T21:07:41.759638Z",
     "iopub.status.idle": "2025-12-07T21:07:41.776575Z",
     "shell.execute_reply": "2025-12-07T21:07:41.775478Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class Fire(nn.Module):\n",
    "    def __init__(self, inplanes, squeeze_planes, expand_planes):\n",
    "        super(Fire, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=1)\n",
    "        self.conv3 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # # MSR initialization\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv2d):\n",
    "        #         n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
    "        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        out1 = self.conv2(x)\n",
    "        out2 = self.conv3(x)\n",
    "        out = torch.cat([out1, out2], 1)\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "\n",
    "class SqueezeNetCIFAR10(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SqueezeNetCIFAR10, self).__init__()\n",
    "        # self.quant = QuantStub()\n",
    "        # self.dequant = DeQuantStub()\n",
    "\n",
    "        # self.upsample = nn.Upsample(size=224, mode='bilinear', align_corners=False)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.fire2 = Fire(96, 16, 64)\n",
    "        self.fire3 = Fire(128, 16, 64)\n",
    "        self.fire4 = Fire(128, 32, 128)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.fire5 = Fire(256, 32, 128)\n",
    "        self.fire6 = Fire(256, 48, 192)\n",
    "        self.fire7 = Fire(384, 48, 192)\n",
    "        self.fire8 = Fire(384, 64, 256)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.fire9 = Fire(512, 64, 256)\n",
    "        self.conv10 = nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "        self.avg_pool = nn.AvgPool2d(13)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.upsample(x)\n",
    "        # x = self.quant(x)\n",
    "        x = self.maxpool1(self.conv1(x))\n",
    "\n",
    "        x = self.fire2(x)\n",
    "        x = self.fire3(x)\n",
    "        x = self.fire4(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.fire5(x)\n",
    "        x = self.fire6(x)\n",
    "        x = self.fire7(x)\n",
    "        x = self.fire8(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = self.fire9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # x = self.dequant(x)\n",
    "        return x\n",
    "    \n",
    "    def load_model(self, path='squeezenet_fp32.pth',device='cpu'):\n",
    "        state_dict = torch.load(path,map_location=device)\n",
    "\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            if k.startswith('module.'):\n",
    "                k = k[len('module.'):]\n",
    "            new_state_dict[k] = v\n",
    "\n",
    "        self.load_state_dict(new_state_dict)\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "\n",
    "        print(f\"Model loaded from {path}\")\n",
    "        # print(self)\n",
    "\n",
    "    def save_model(self, path='squeezenet_fp32.pth'):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        print(f\"Model saved to {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6a2464",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "In this cell we define a function to load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dadba32b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T21:07:41.780071Z",
     "iopub.status.busy": "2025-12-07T21:07:41.779880Z",
     "iopub.status.idle": "2025-12-07T21:07:41.783912Z",
     "shell.execute_reply": "2025-12-07T21:07:41.783449Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(path='./data', batch_size=64):\n",
    "  print(\"Loading the CIFAR10 dataset\")\n",
    "\n",
    "  transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(), # scale RGB 0-255 to 0-1\n",
    "    # normalize with known mean and std deviation of the CIFAR10 dataset\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))\n",
    "  ])\n",
    "\n",
    "  # train_transform = transforms.Compose([\n",
    "  #   transforms.RandomCrop(32, padding=4),\n",
    "  #   transforms.RandomHorizontalFlip(),\n",
    "  #   transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
    "  #   transforms.ToTensor(),\n",
    "  #   transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "  #                   (0.2023, 0.1994, 0.2010)),\n",
    "  # ])\n",
    "  train_transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # Resize before any augmentation\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "  ])\n",
    "\n",
    "  # get training data\n",
    "  train_dataset = datasets.CIFAR10(root=path, train=True, download=True, transform=train_transform)\n",
    "  # get test data\n",
    "  test_dataset = datasets.CIFAR10(root=path, train=False, download=True, transform=transform)\n",
    "  # load the training data\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,num_workers=8,pin_memory=True)\n",
    "  # load the test data\n",
    "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,num_workers=8,pin_memory=True)\n",
    "\n",
    "  print(f\"Loaded train data: {len(train_loader.dataset)} total samples, {len(train_loader)} batches\\n\"\n",
    "      f\"Loaded test data: {len(test_loader.dataset)} total samples, {len(test_loader)} batches\")\n",
    "\n",
    "  return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2d80055",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T21:07:41.787597Z",
     "iopub.status.busy": "2025-12-07T21:07:41.787287Z",
     "iopub.status.idle": "2025-12-07T21:07:57.481399Z",
     "shell.execute_reply": "2025-12-07T21:07:57.480752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the CIFAR10 dataset\n",
      "Loaded train data: 50000 total samples, 782 batches\n",
      "Loaded test data: 10000 total samples, 157 batches\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = load_dataset(batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb673199",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "In the cells below we define a function to visualize our training and train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82cd0d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T21:07:57.491380Z",
     "iopub.status.busy": "2025-12-07T21:07:57.490749Z",
     "iopub.status.idle": "2025-12-07T21:07:58.132801Z",
     "shell.execute_reply": "2025-12-07T21:07:58.131917Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(metrics):\n",
    "  train_losses = metrics.get('train_loss',None)\n",
    "  test_losses = metrics.get('test_loss',None)\n",
    "  train_accs = metrics.get('train_acc',None)\n",
    "  test_accs = metrics.get('test_acc',None)\n",
    "\n",
    "  epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "  plt.figure(figsize=(12, 5))\n",
    "\n",
    "  # Loss Graph\n",
    "  plt.subplot(1, 2, 1)\n",
    "  if train_losses:\n",
    "    plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
    "  if test_losses:\n",
    "    plt.plot(epochs, test_losses, label='Test Loss', marker='s')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.title('Training vs Test Loss')\n",
    "  plt.legend()\n",
    "  plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "  # Accuracy Graph\n",
    "  plt.subplot(1, 2, 2)\n",
    "  if train_accs:\n",
    "    plt.plot(epochs, train_accs, label='Train Accuracy', marker='o')\n",
    "  if test_accs:\n",
    "    plt.plot(epochs, test_accs, label='Test Accuracy', marker='s')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Accuracy (%)')\n",
    "  plt.title('Training vs Test Accuracy')\n",
    "  plt.legend()\n",
    "  plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c9f93e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T21:07:58.137437Z",
     "iopub.status.busy": "2025-12-07T21:07:58.137153Z",
     "iopub.status.idle": "2025-12-07T21:07:58.143337Z",
     "shell.execute_reply": "2025-12-07T21:07:58.142866Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model,train_loader,test_loader,train=True,test=True,device='cpu',epochs=10,lr=1e-3):\n",
    "  model.to(device)\n",
    "  metrics = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "  # TRAINING LOOP\n",
    "  optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "  criterion_test = nn.CrossEntropyLoss()\n",
    "  # optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "  # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "\n",
    "  for e in range(epochs):\n",
    "    print(f\"Epoch [{e+1}/{epochs}] \",end='')\n",
    "    if train:\n",
    "      model.train()\n",
    "      train_loss, total_examples, correct = 0.0, 0, 0\n",
    "\n",
    "      for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True) # zero gradients\n",
    "        outputs = model(inputs) # forward pass\n",
    "        loss = criterion(outputs,labels) # get loss from cost function\n",
    "        loss.backward() # backward propagation\n",
    "        optimizer.step() # update gradients\n",
    "\n",
    "        # train_loss += loss.item() # track total loss up to this point\n",
    "        train_loss += loss.item() * labels.size(0)\n",
    "        _, pred_ind = outputs.max(1) # get index of prediction (highest value)\n",
    "        total_examples += labels.size(0) # update count for this epoch with batch size\n",
    "        correct += pred_ind.eq(labels).sum().item() # return count of correct predictions\n",
    "\n",
    "      # scheduler.step() \n",
    "    #   train_loss /= len(train_loader) # get average per batch\n",
    "      train_loss /= total_examples # get average per example\n",
    "      train_acc = 100.0 * correct / total_examples\n",
    "\n",
    "      metrics[\"train_loss\"].append(train_loss)\n",
    "      metrics[\"train_acc\"].append(train_acc)\n",
    "\n",
    "      print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% \",end='')\n",
    "\n",
    "      # VALIDATION/TEST\n",
    "    if test:\n",
    "      model.eval()\n",
    "      test_loss, total_examples, correct = 0.0, 0, 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "          outputs = model(inputs) # forward pass\n",
    "          loss = criterion_test(outputs,labels) # get loss from cost function\n",
    "          test_loss += loss.item() * labels.size(0) # update loss\n",
    "          _, pred_ind = outputs.max(1) # get index of prediction (highest value)\n",
    "          total_examples += labels.size(0) # update count for this epoch with batch size\n",
    "          correct += pred_ind.eq(labels).sum().item() # return count of correct predictions\n",
    "\n",
    "      test_loss /= total_examples\n",
    "      test_acc = 100.0 * correct / total_examples\n",
    "\n",
    "      metrics[\"test_loss\"].append(test_loss)\n",
    "      metrics[\"test_acc\"].append(test_acc)\n",
    "\n",
    "      print(f\"Test/Val Loss: {test_loss:.4f}, Test/Val Acc: {test_acc:.2f}%\")\n",
    "\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de683d9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T21:07:58.147669Z",
     "iopub.status.busy": "2025-12-07T21:07:58.147223Z",
     "iopub.status.idle": "2025-12-07T21:07:58.179995Z",
     "shell.execute_reply": "2025-12-07T21:07:58.179293Z"
    }
   },
   "outputs": [],
   "source": [
    "# def init_weights_he(m):\n",
    "#     if isinstance(m, nn.Conv2d):\n",
    "#         nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "#         if m.bias is not None:\n",
    "#             nn.init.zeros_(m.bias)\n",
    "\n",
    "#     elif isinstance(m, nn.Linear):\n",
    "#         nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "#         nn.init.zeros_(m.bias)\n",
    "        \n",
    "# model_fp32 = SqueezeNetCIFAR10()\n",
    "# model_fp32.apply(init_weights_he)\n",
    "# # model_fp32.load_model('squeezenet_fp32.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec7c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = True, True\n",
    "# epochs = 100\n",
    "# fp32_metrics = train_model(model=model_fp32,train_loader=train_loader,test_loader=test_loader,train=train,test=test,device=device,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67bb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fp32.save_model(\"squeezenet_fp32.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0080d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metrics(fp32_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcce7a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader,device='cpu'):\n",
    "  model.eval()\n",
    "  model.to(device)\n",
    "  correct, total = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for images, labels in test_loader:\n",
    "          images = images.to(device, non_blocking=True)\n",
    "          labels = labels.to(device, non_blocking=True)\n",
    "          outputs = model(images)\n",
    "          _, pred = outputs.max(1)\n",
    "          correct += pred.eq(labels).sum().item()\n",
    "          total += labels.size(0)\n",
    "\n",
    "  acc = 100.0 * correct / total\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afab91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = evaluate(model_fp32,test_loader,device)\n",
    "# print(f\"FP32 Test Accuracy: {acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0abd46",
   "metadata": {},
   "source": [
    "## Task 2: Quantize Squeezenet\n",
    "\n",
    "For task 2 we use quantization aware training to quantize SqueezeNet to INT8. After training for 50 epochs we are able to achieve a final test accuracy of 69.16% with the quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9edbee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 08:46:00.465000 7340 site-packages\\torch\\utils\\cpp_extension.py:466] Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['where', 'cl']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mqtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FixedPoint, FloatingPoint\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mqtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Quantizer\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Define target fixed-point format (ap_fixed<8,4>)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 8 total bits, 4 fractional bits → Q3.4\u001b[39;00m\n\u001b[32m      8\u001b[39m forward_num = FixedPoint(wl=\u001b[32m8\u001b[39m, fl=\u001b[32m4\u001b[39m, rounding=\u001b[33m\"\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m\"\u001b[39m, saturate=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\qtorch\\quant\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquant_function\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquant_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      4\u001b[39m __all__ = [\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfixed_point_quantize\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mblock_quantize\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mQuantizer\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\qtorch\\quant\\quant_function.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      9\u001b[39m current_path = os.path.dirname(os.path.realpath(\u001b[34m__file__\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m quant_cpu = \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquant_cpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquant_cpu/quant_cpu.cpp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquant_cpu/bit_helper.cpp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquant_cpu/sim_helper.cpp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n\u001b[32m     20\u001b[39m     quant_cuda = load(\n\u001b[32m     21\u001b[39m         name=\u001b[33m\"\u001b[39m\u001b[33mquant_cuda\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m         sources=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m         ],\n\u001b[32m     31\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:1681\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, sources, extra_cflags, extra_cuda_cflags, extra_sycl_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, with_sycl, is_python_module, is_standalone, keep_intermediates)\u001b[39m\n\u001b[32m   1572\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(name,\n\u001b[32m   1573\u001b[39m          sources: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[32m   1574\u001b[39m          extra_cflags=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1584\u001b[39m          is_standalone=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1585\u001b[39m          keep_intermediates=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1586\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1587\u001b[39m \u001b[33;03m    Load a PyTorch C++ extension just-in-time (JIT).\u001b[39;00m\n\u001b[32m   1588\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1679\u001b[39m \u001b[33;03m        ...     verbose=True)\u001b[39;00m\n\u001b[32m   1680\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1681\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jit_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1683\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43msources\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_get_build_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_python_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_intermediates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_intermediates\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2138\u001b[39m, in \u001b[36m_jit_compile\u001b[39m\u001b[34m(name, sources, extra_cflags, extra_cuda_cflags, extra_sycl_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, with_sycl, is_python_module, is_standalone, keep_intermediates)\u001b[39m\n\u001b[32m   2134\u001b[39m                 hipified_sources.add(hipify_result[s_abs].hipified_path \u001b[38;5;28;01mif\u001b[39;00m s_abs \u001b[38;5;129;01min\u001b[39;00m hipify_result \u001b[38;5;28;01melse\u001b[39;00m s_abs)\n\u001b[32m   2136\u001b[39m             sources = \u001b[38;5;28mlist\u001b[39m(hipified_sources)\n\u001b[32m-> \u001b[39m\u001b[32m2138\u001b[39m         \u001b[43m_write_ninja_file_and_build_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2139\u001b[39m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2140\u001b[39m \u001b[43m            \u001b[49m\u001b[43msources\u001b[49m\u001b[43m=\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2141\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2142\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2143\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2144\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2145\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2146\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2147\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2148\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2149\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2150\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2151\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m verbose:\n\u001b[32m   2152\u001b[39m     logger.debug(\u001b[33m'\u001b[39m\u001b[33mNo modifications detected for re-loaded extension module \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, skipping build step...\u001b[39m\u001b[33m'\u001b[39m, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2275\u001b[39m, in \u001b[36m_write_ninja_file_and_build_library\u001b[39m\u001b[34m(name, sources, extra_cflags, extra_cuda_cflags, extra_sycl_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, with_sycl, is_standalone)\u001b[39m\n\u001b[32m   2271\u001b[39m     os.makedirs(build_directory, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2273\u001b[39m \u001b[38;5;66;03m# NOTE: Emitting a new ninja build file does not cause re-compilation if\u001b[39;00m\n\u001b[32m   2274\u001b[39m \u001b[38;5;66;03m# the sources did not change, so it's ok to re-emit (and it's fast).\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2275\u001b[39m \u001b[43m_write_ninja_file_to_build_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuild_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2278\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m=\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m   2289\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[33mBuilding extension module \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m'\u001b[39m, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2762\u001b[39m, in \u001b[36m_write_ninja_file_to_build_library\u001b[39m\u001b[34m(path, name, sources, extra_cflags, extra_cuda_cflags, extra_sycl_cflags, extra_ldflags, extra_include_paths, with_cuda, with_sycl, is_standalone)\u001b[39m\n\u001b[32m   2759\u001b[39m ext = EXEC_EXT \u001b[38;5;28;01mif\u001b[39;00m is_standalone \u001b[38;5;28;01melse\u001b[39;00m LIB_EXT\n\u001b[32m   2760\u001b[39m library_target = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2762\u001b[39m \u001b[43m_write_ninja_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2765\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpost_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2766\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcuda_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcuda_flags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2767\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcuda_post_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcuda_dlink_post_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2769\u001b[39m \u001b[43m    \u001b[49m\u001b[43msycl_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43msycl_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2770\u001b[39m \u001b[43m    \u001b[49m\u001b[43msycl_post_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2771\u001b[39m \u001b[43m    \u001b[49m\u001b[43msycl_dlink_post_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43msycl_dlink_post_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2772\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m=\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2773\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2774\u001b[39m \u001b[43m    \u001b[49m\u001b[43mldflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mldflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2775\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2946\u001b[39m, in \u001b[36m_write_ninja_file\u001b[39m\u001b[34m(path, cflags, post_cflags, cuda_cflags, cuda_post_cflags, cuda_dlink_post_cflags, sycl_cflags, sycl_post_cflags, sycl_dlink_post_cflags, sources, objects, ldflags, library_target, with_cuda, with_sycl)\u001b[39m\n\u001b[32m   2944\u001b[39m link_rule = [\u001b[33m'\u001b[39m\u001b[33mrule link\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IS_WINDOWS:\n\u001b[32m-> \u001b[39m\u001b[32m2946\u001b[39m     cl_paths = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwhere\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2947\u001b[39m \u001b[43m                                        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.decode(*SUBPROCESS_DECODE_ARGS).split(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m   2948\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cl_paths) >= \u001b[32m1\u001b[39m:\n\u001b[32m   2949\u001b[39m         cl_path = os.path.dirname(cl_paths[\u001b[32m0\u001b[39m]).replace(\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m$:\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:472\u001b[39m, in \u001b[36mcheck_output\u001b[39m\u001b[34m(timeout, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    469\u001b[39m         empty = \u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    470\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33minput\u001b[39m\u001b[33m'\u001b[39m] = empty\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m           \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.stdout\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:577\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    575\u001b[39m     retcode = process.poll()\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    578\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['where', 'cl']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from qtorch import FixedPoint, FloatingPoint\n",
    "from qtorch.quant import Quantizer\n",
    "\n",
    "# Define target fixed-point format (ap_fixed<8,4>)\n",
    "# 8 total bits, 4 fractional bits → Q3.4\n",
    "forward_num = FixedPoint(wl=8, fl=4, rounding=\"nearest\", saturate=True)\n",
    "\n",
    "# Use standard FP32 for backward gradients\n",
    "backward_num = FloatingPoint(exp=8, man=23)  # 32-bit float\n",
    "\n",
    "# Create a quantizer\n",
    "Q = Quantizer(forward_number=forward_num,\n",
    "              backward_number=backward_num,\n",
    "              forward_rounding=\"nearest\",\n",
    "              backward_rounding=\"stochastic\")\n",
    "\n",
    "def add_weight_quant(module):\n",
    "    \"\"\"\n",
    "    Recursively add weight quantization to Conv2d/Linear layers.\n",
    "    Stores original float weights as 'weight_fp'.\n",
    "    \"\"\"\n",
    "    for name, child in module.named_children():\n",
    "        add_weight_quant(child)\n",
    "\n",
    "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "        if not hasattr(module, 'weight_fp'):\n",
    "            module.weight_fp = nn.Parameter(module.weight.data.clone())\n",
    "        \n",
    "        # Override forward to quantize weights\n",
    "        orig_forward = module.forward\n",
    "        def forward_hook(x, module=module, orig_forward=orig_forward):\n",
    "            module.weight.data = Q(module.weight_fp)\n",
    "            return orig_forward(x)\n",
    "        \n",
    "        module.forward = forward_hook\n",
    "\n",
    "def apply_activation_q(model):\n",
    "    for name, child in model.named_children():\n",
    "        apply_activation_q(child)\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            # replace inplace ReLU with non-inplace sequential\n",
    "            new_relu = nn.Sequential(nn.ReLU(inplace=False), Q)\n",
    "            setattr(model, name, new_relu)\n",
    "\n",
    "class SqueezeNetFixedQAT(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32 = SqueezeNetCIFAR10()\n",
    "model_fp32.load_model(\"squeezenet_fp32_final.pth\")\n",
    "\n",
    "# Wrap for fixed-point QAT\n",
    "model_qat = SqueezeNetFixedQAT(model_fp32)\n",
    "\n",
    "# Apply activation quantization\n",
    "apply_activation_q(model_qat.model)\n",
    "\n",
    "# Apply weight quantization\n",
    "add_weight_quant(model_qat.model)\n",
    "\n",
    "# model_qat = SqueezeNetCIFAR10()\n",
    "# model_qat.model.load_model('squeezenet_fp32_final.pth')\n",
    "# add_weight_quant_hooks(model_qat)\n",
    "# apply_activation_q(model_qat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a2fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_metrics = train_model(model=model_qat,train_loader=train_loader,test_loader=test_loader,device=device,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c676051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(qat_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f06cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qat.eval()\n",
    "torch.save(model_qat.state_dict(), \"squeezenet_stubs.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = evaluate(model_qat,test_loader,device)\n",
    "print(f\"Fixed Point Test Accuracy: {acc}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
